# listtools

Tools for manipulating lists of things, and in particular, taxonomic
checklists and hierarchies.

This tool suite is intended to eventually replace the ['checklist
diff'](https://github.com/jar398/cldiff) tool.  Although most
list comparison functionality is in place, the logic for comparing
hierarchies is still being debugged.

## Overview

The 'tools' or 'scripts' or 'programs' in this repository manipulate
tabular data in the form of CSV files.  Most of them act as filters
and can be invoked from the Unix shell.  They can be sequenced into
pipelines using '|' if desired.  CSV files uniformly have a single
header row giving column names.

I have found it convenient to run the tools by using `make` applied to
a suitable 'makefile'.  This way, intermediate objects can be cached
in the local file system, avoiding expensive regeneration steps when
inputs have not changed.

For my testing I use python 3, `bash`, and GNU `make` exclusively, all
running on Debian Linux.

## Terminology - usages and taxa

Many of the tools are completely generic over tabular data, but a few
are specific to biodiversity information in the form of "Darwin Core"
files.

When I speak of a Darwin core (DwC) file I take this to mean (for
purposes of these tools) a CSV file where each row has information
connected to a name, such as a Linnaean binomial.  The row is to be
interpreted in the context of all the other rows in the same file and
what they say about one another, and in the context of whatever we
know about the origin of the file itself.  A row refers to the way the
name is used or its "usage", roughly following the TDWG term
"taxonomic name usage" or TNU.

In some files a single name (string) might be used in multiple ways
(homonyms), but if so each "way" or usage will have its own row.

The primary key in such files is `taxonID`, which is a misnomer.
Certainly such a key identifies a row in the file, and the row has an
associated usage, so we might say the `taxonID` "identifies" a usage,
but whether a usage determines a "taxon" is a matter of debate.  In
the case of synonyms, analysis could show that multiple "usages" might
correspond to a single taxon, so multiple `taxonID`s could correspond
to the same taxon, bringing the "taxon identifier" notion into doubt.
On the other hand, while a primary identifier (a `taxonID`) always
corresponds to a usage, the usage may not be clear or specific enough
to identify any particular taxon.

Further confusing things, a primary key (`taxonID`) is only unique
within a particular file.

Other Darwin Core columns containing row or usage identifiers have
column names that contain 'usage' as a morpheme,
e.g. `parentNameUsageID`.

In the EOL internals, usage rows (records) are called "nodes".  I
avoid the word "node" due to its various conflicting and restricting
meanings, such as for graph database nodes in Neo4J.


## Installation

Python 3, `bash`, and Gnu `make` are all assumed.

Install the python3 `regex` module (although it's easy to revert to
the built-in `re` if you prefer not to).

One of the `Makefile` rules uses `gnparser`, which has download
instructions [here](https://github.com/gnames/gnparser#installation).
`Makefile` assumes that `gnparser` can be located via your shell's `PATH`.
It is easy to remove this dependency if desired, with only a mild
degradation in effectiveness.

## The tools

### start

Any Darwin Core data source should be run through src/start.py first,
for some modest validation and DwC-specific cleaning.

    src/start.py --pk taxonID <foo-raw.tsv >foo.csv

`--pk` specifies the column containing the table's primary keys.  If
there is no such column, one is added, and if a primary key is
missing, a new one is generated by hashing the contents of the row.

Reused putatively-primary keys are detected and flagged.

Data cleaning is performed for the following columns:

 - `canonicalName`, `scientificName` - if a scientific name (with
   authority) is found in the canonicalName column, and the
   `scientificName` column is empty, then the scientific name is moved
   to the `scientificName` column (and so on)
 - `acceptedNameUsageID` - if empty, copy the `taxonID`
 - `taxonomicStatus` - flag if a record with taxonomic status
   `synonym` does not have an accepted usage id, or if one with status
   `accepted` does
 - `source` - cleanup specific to EOL DH 0.9 and smasher - remove
   source record ids other than the first
 - `Landmark` - recode values, change to `landmark_status` - EOL specific cleanup


### sortcsv

`sortcsv` sorts standard input by the contents of the given `--pk`
column, and writes the result to standard output.

    src/sortscv.py --pk taxonID <foo.csv >foo-sorted.csv


### project

`project` drops columns from the input.
With `--keep`, it drops all columns other than those specified:

    src/project.py --keep a,b,c <foo.csv >less-foo.csv

With `--drop`, it drops particular columns, keeping all the rest:

    src/project.py --drop a,b,c <foo.csv >less-foo.csv

### match_records

Given input checklists A and B, finds the best unique mutual matches
between the A records and the B records.

 * `--target` filename  - the B input.
 * `--pk` K - specify primary key; default `taxonID`
 * `--index` columns names - lists columns to be used for comparison,
      in priority order.

Standard input is the A input.

The output (to standard output) has these columns:

 - `taxonID` - this is a unique primary key for the row.  If there is
   a B usage given, then this id is the same as that for the B usage.
   If not, it is the A id, if it doesn't conflict with any B id, or
   freshly generated otherwise.
   Always present.
 - `taxonID_A` - the id of an A usage, or empty if the row corresponds
   to an unmatched B usage record.
 - `taxonID_B` - the id of a B usage, or empty if the row corresponds
   to an unmatched A usage record.
 - `remark` - a textual report of what happened, if no match was made.

If the primary key is specified as something different, the first
three columns reflect what's given.

Records are matched by the values in their 'index' columns, which
should be given in priority order.  That is, if the index columns are
x, y, and z, then an attempt to match records is made first on the
values in the x column.  If either value is missing, or if the
match is ambiguous, then the y column is consulted, and so on.

In the case of an ambiguity, i.e. multiple A records matching a B
record with the same match score or vice versa, none of the records is
matched.  This fact is recorded in the `remark` column of the delta.
A situation like this should provoke manual review; it could mean
that the inputs are ill-formed, or it could mean the scoring algorithm
simply doesn't have enough information to choose between the matches.

The B input should not contain a `mode` column, and if it does, the
column is ignored.


### delta

`delta` compares two files (call them A and B), matching rows of one
to rows of the other, and generating a "delta" (call it B-A) which
describes the differences between A and B.  A delta is an annotated
report listing all unmatched and changed rows, not including exact row
matches.

    src/delta.py <a.csv --target b.csv \
                        --matches m.csv \
                        --pk y \
                        --index x,y,z \
                        --managed z,y,d,e,f

`--matches` names a file that was generated by `match_records` (or
another tool generating the same kind of file).  If it's omitted then
`match_records` is invoked implicitly.

`--pk` specifies the primary key column for both inputs.

Each row of the delta comes from A only, from B only, or from matched
rows in A and B.  Because of the use of deltas in patching, these
three types of row are given labels `remove`, `add`, and `update` in
the `mode` column of the delta.  The primary key in the delta is taken
from A in the case of `remove` and `update` records.  The primary key
from the B file is given in `new_pk` of the delta.

The output contains only the managed columns (`--managed`), and matched
rows are considered updated only if one of the managed columns
differs.

The matches are done on only a row-by-row basis and are not sensitive
to hierarchy or other sources of meaning.  Hierarchy and
synonym-to-accepted links are treated the same as any other field and
do not require identical contents, meaning that the overall comparison
is not truly sensitive to hierarchy.  Usage rows may be matched even
when consideration of hierarchy would require them to be interpreted
as distinct taxa.  For hierarchy sensitive comparison see `align`, below.

### apply

Applies a sorted delta, B-A, to a sorted file A (which typically would be
the A file from which the delta was generated), generating a file B′.
B′ will be projection of B to the given 'managed' columns, with
perhaps the rows in a different order.

    src/apply.py --delta ba.delta --pk taxonID \
                 < a.csv > b2.csv

The delta would probably be produced by the delta tool.  It should
have columns `mode` and `new_pk` (see above).

### scatter

Given a delta, generate a directory containing one file for each of
the three kinds of record in the delta (add, udpate, remove).

    src/scatter.py <delta.csv --dest delta

writes `delta/add.csv`, `delta/update.csv`, `delta/remove.csv`.  These
can be fed to appropriate database commands to incrementally update a
database to a new version of a table (intended for EOL mainly).

### subset

`subset` generates a subset of the rows of a given file, starting from
a specified root.

    src/subset.py --root 40674 <all.csv >some.csv

It assumes the usual Darwin Core hierarchical structure, given by
these columns: 

 - `taxonID` identifies a usage row
 - `parentNameUsageID` indicates the usage row for the parent in the hierarchy
 - `acceptedNameUsageID` indicates an accepted usage that can replace an
   non-accepted one.
 - `taxonomicStatus` is `accepted` for an accepted (non-synonym) row, 
   `synonym` for a synonym row

### hierarchy

This is specific to EOL.  It applies a usage id to taxon id ('page id') mapping
(in a file named by an argument) to a file full of usages to generate a file
with one row per taxon, giving the parent of each taxon.

The resulting taxon list can be subjected to `delta` and `scatter` to
incrementally update an in-database hierarchy, etc.

Columns expected: `taxonID`, `parentNameUsageID`,
`acceptedNameUsageID`, `taxonomicStatus`

### ncbi_to_dwc

Converts an NCBI taxdmp .zip file to DwC.  For example, suppose we fetch
a zip file and unzip it, e.g.:

    wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump_archive/taxdmp_2015-05-01.zip
    unzip -d dump taxdmp_2015-05-01.zip

Then we can convert the dump directory to a single DwC .csv file:

    src/ncbi_to_dwc.py dump >taxdmp_2015-05-01.csv

Columns in the DwC output:
`taxonID`, `NCBI Taxonomy ID`, `parentNameUsageID`, `taxonRank`,
`acceptedNameUsageID`, `scientificName`, `canonicalName`,
`taxonomicStatus`, `nomenclaturalStatus`

### newick

An extremely rudimentary Newick notation parser.  

The following
accepts a Newick string on the command line and emits a CSV table with
columns `taxonID`, `parentNameUsageID`, and `canonicalName`:

    src/newick.py "(a,b)c"

The following reads CSV (with header row) from standard input and
writes a Newick string to standard output:

    src/newick.py

Branch length syntax is not handled.  Newick escape sequences are not handled.

TBD: be able to take a Newick string from standard input.

### align

Hierarchy alignment, seeded with record matches (`match_records`,
above).  Work in progress.

 - Input files: A; B; A/B record matches (see `match_records`)
 - Standard output: the 'sum' of A and B.
   Columns:
     - `taxonID` - a unique primary key within the output.  Usually but not 
       always either the A taxonID or the B taxonID.
     - `taxonID_A` - a taxonID from the A input file, or empty
     - `taxonID_B` - a taxonID from the B input file, or empty
     - `parentNameUsageID` - a taxonID in the output, or empty at the root(s)
     - `acceptedNameUsageID` - a taxonID in the output, when the row is a synonym.
     - `canonicalName` - this is mainly for debugging and ease of inspection.
       A `canonicalName` copied from the B input, if taxonID_B is present,
       or from the A input otherwise.
     - `remark` - annotation concerning how the alignment was made, or why it wasn't

Invocation:

    src/align.py < A.csv --target B.csv --matches AB_RM.csv > AB.csv

where `AB_RM.csv` was generated by `match_records`.

### util

This module is not invoked on its own, but just contains a few
internal utilities shared among the various tools.


## Demos

`make` will compare the mammals in two versions of NCBI Taxonomy.
Read the comments in the `Makefile` for other things to try.

## Reading the output

I use a few conventions for the progress output generated by the tools:

 - Lines beginning with a run of hyphens `--` are intended to be read by the
   general user, and describe what is going on in terms that ought to
   be easily understood
 - Lines beginning with asterisk `*` have to do with the detection of bugs or
   peculiar or confusing situations that might demand some attention
 - Lines beginning with a hash `#` are directed at me, and I don't
   expect anyone else to understand them
 - Other lines come from code that I didn't write, such as `make` or `wc`


## Configuration for EOL

For EOL, the list tools are intended to be used in conjunction with
`plotter`, which provides access to file repositories such as
`content.eol.org` and other sources of EOL information.

Configure `plotter` according to its documentation.  Then, assuming
the `plotter` repository is cloned in directory `../plotter`, do the
following in your `listtools` working directory:

    ln -sfT ../plotter/config config

Depending on the location of `plotter` and the directory where you're
working, adjust the above command, and also update `Makefile` where it
mentions `plotter`.
