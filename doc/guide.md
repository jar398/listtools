# List tools user guide

See also [philosophy / theory / method](theory.md)

## Overview

The 'tools' or 'scripts' or 'programs' in this repository manipulate
tabular data in the form of CSV files.  Most of them act as filters
and can be invoked from the Unix shell.  They can be sequenced into
pipelines using '|' if desired.  CSV files uniformly have a single
header row giving column names.

I have found it convenient to run the tools by using `make` applied to
a suitable `makefile'.  This way, intermediate objects can be cached
in the local file system, avoiding expensive regeneration steps when
inputs have not changed.

## Installation
<a name="installation"/>

The tools require no particular installation step.  They can be
invoked from the shell out of a clone of this `git` repository.

Python 3 and `bash` are assumed.

Install the python3 `regex` module (although it's easy to revert to
the built-in `re` if you prefer not to).

Also required is `gnparser`, which has download instructions
[here](https://github.com/gnames/gnparser#installation).


## The tools

To learn about a given tool, please supplement what follows with
whatever you learn from using the `--help` option, e.g.

    $ src/clean.py --help

### clean

A Darwin Core data source should be run through src/clean.py first,
for some modest validation and DwC-specific cleaning.

    src/clean.py --pk taxonID --input foo-raw.tsv >foo-clean.csv

`--pk` specifies the column containing primary keys.  If
there is no such column, one is added, and if a primary key is
missing, a new one is generated by hashing the contents of the row.

Values in the primary key column that occur there more than once
are detected and flagged.

`--input` followed by a file name specifies the location of the
uncleaned Darwin Core file.  This is assumed to be in CSV
(comma-separated values) format if the file name ends in `.csv` and
TSV format otherwise.  The output is always CSV.

Data cleaning is performed for the following columns:

 - `canonicalName`, `scientificName` - if a scientific name (one that has an
   authority) is found in the `canonicalName` column, and the
   `scientificName` column is empty, then the scientific name is moved
   to the `scientificName` column.  Similarly, if a non-scientific name
   is found in the `scientificName` column, it's moved to the `canonicalName` column.
 - `acceptedNameUsageID` - if it just replicates the `taxonID`, clear it
 - `taxonomicStatus` - flag if a record with taxonomic status
   `synonym` does not have an accepted taxon id, or if one with status
   `accepted` does
 - `source` - cleanup specific to EOL DH 0.9 and smasher - remove
   source record ids other than the first
 - `Landmark` - recode values, change to `landmark_status` - EOL specific cleanup

`--managed prefix:column` is for designating use of managed identifier
spaces.  If one column contains, say, NCBI taxids, or GBIF taxon
identifiers, or anything similar, that are stable across versions of
the source (as opposed to being idiosyncratic to one version), then
the column should be copied to the `managed_id` column.  This
operation is what this feature is for.  For examples, `--managed
gbif:taxonID` means that the taxonID column contains managed GBIF
taxon 'identifiers' and the `managed_id` column will contain 'managed
identifiers' (an idea I made up).  E.g. if a row's taxonID contains
`359` then the string `gbid:359` will be placed in the `managed_id`
column.  This will then be used for matching operations.


### project

`project` (emphasis on second syllable) drops columns from the input.
With `--keep`, it drops all columns other than those specified:

    src/project.py --keep a,b,c <foo.csv >less-foo.csv

With `--drop`, it drops particular columns, keeping all the rest:

    src/project.py --drop a,b,c <foo.csv >less-foo.csv


### extract_names

This extracts `scientificName`s in a form suitable for consumption by `gnparse`.
If there is no `scientificName` then the `canonicalName` is extracted.


### use_gnparse

This consumes the output of `gnparser` and combines it with the table
that was just processed, enriching the table with the addition of 
columns from the `gnparser` output.


### exemplar

Given input checklists A and B, finds the best mutual matches
between the A records and the B records.

 * `--A` filename  - the A checklist.
 * `--B` filename  - the B checklist.

The output (to standard output) has these columns:

 - `match_id` - the id of an A record, or empty if the row corresponds
   to an unmatched B record.
 - `relation` - an RCC5 relationship, but really either `=`, `?` (no
   information), or empty (nothing to compare to).
   (Mistake, that ought to be `relationship`.)
 - `taxonID` - the id of a B record, or empty if the row corresponds
   to an unmatched A record.
 - `basis_of_match` - a textual report of what happened, with
   the deciding column name is a match was made, and a diagnostic
   if no match was made and there was a near miss.

If the primary key is specified as something different, the first
three columns reflect what's given.

Records are matched by the values in their 'index' columns, which
should be given in priority order.  That is, if the index columns are
x, y, and z, then an attempt to match records is made first on the
values in the x column.  If either value is missing, or if the
match is ambiguous, then the y column is consulted, and so on.

In the case of an ambiguity, i.e. multiple A records matching a B
record with the same match score or vice versa, none of the records is
matched.  This fact is recorded in the `remark` column of the delta.
A situation like this should provoke manual review; it could mean
that the inputs are ill-formed, or it could mean the scoring algorithm
simply doesn't have enough information to choose between the matches.

The B input should not contain a `mode` column, and if it does, the
column is ignored.

### ncbi_to_dwc

Converts an NCBI taxdmp .zip file to DwC.  For example, suppose we fetch
a zip file and unzip it, e.g.:

    wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump_archive/taxdmp_2015-05-01.zip
    unzip -d dump taxdmp_2015-05-01.zip

Then we can convert the dump directory to a single DwC .csv file:

    src/ncbi_to_dwc.py dump >taxdmp_2015-05-01.csv

Columns in the DwC output:
`taxonID`, `NCBI Taxonomy ID`, `parentNameUsageID`, `taxonRank`,
`acceptedNameUsageID`, `scientificName`, `canonicalName`,
`taxonomicStatus`, `nomenclaturalStatus`

### subset

`subset` generates a subset of the rows of a given file, starting from
a specified root.

    src/subset.py --root 40674 <all.csv >some.csv

It assumes the usual Darwin Core hierarchical structure, given by
these columns: 

 - `taxonID` identifies a row (and a taxon, which is probably an extension)
 - `parentNameUsageID` indicates the record for the parent in the hierarchy
 - `acceptedNameUsageID` indicates an accepted name that might replace an
   non-accepted one (although in fact there is more meaning to this attribute).
 - `taxonomicStatus` is `accepted` for an accepted (non-synonym) row, 
   `synonym` for a synonym row

You can specify the root using its `canonicalName` it that's unique:

    src/subset.py --root Mammalia <all.csv >some.csv

### sortcsv

`sortcsv` sorts standard input by the contents of the given `--pk`
(pk = primary key) column, and writes the result to standard output.

    src/sortscv.py --pk taxonID <foo.csv >foo-sorted.csv


### demo

Generates an Euler/X input file with RCC5 articulations gleaned from a
comparison of two input checklists.

[To be documented - use `--help`]


### newick

An extremely rudimentary Newick notation parser.  

The following accepts a Newick string on the command line and emits a
CSV table with basic Darwin Core columns:

    $ src/newick.py --newick "(a,b)c"
    taxonID,canonicalName,parentNameUsageID,acceptedNameUsageID,taxonomicStatus
    2,a,1,,accepted
    3,b,1,,accepted
    1,c,,,accepted
    $ 

The following reads CSV (with header row) from standard input and
writes a Newick string to standard output:

    $ cat <<EOF | src/newick.py
    taxonID,canonicalName,parentNameUsageID,acceptedNameUsageID,taxonomicStatus
    2,a,1,,accepted
    3,b,1,,accepted
    1,c,,,accepted
    EOF
    (a,b)c
    $ 

Branch length syntax is not handled.  Newick escape sequences are not handled.

This program supports an idiosyncratic syntax for synonyms (useful
since the main use for this feature is testing): an asterisk `*`
suffixed to a name says that the name is to be considered a synonym
(i.e. not accepted).


## Deprecated

### merge

<strong>Obsolete for now.</strong>

Replaces `align` TBD

Invocation:

    src/merge.py < A.csv --B B.csv --matches AB_RM.csv > AB.csv

where `AB_RM.csv` was generated by `match_records`.

[in disrepair, superseded by `span` module]

### align

[in disrepair. do not use]

Generates an RCC5 alignment of two checklists.

[To be documented]

### report

<strong>Obsolete for now.</strong>

Input: the output of `merge`.

Two outputs:

 * per-taxon details comparing them between the two inputs, and
 * a summary report breaking down all taxa by the nature of any
   difference.  Here is an example of a summary:

```
     37      37 accepted/synonym
    326     325 canonicalName
     40      40 canonicalName;accepted/synonym
     15      15 canonicalName;synonym/accepted;use of name (in A ? in B)
     23      23 canonicalName;use of name (in A < in B)
      7       6 canonicalName;use of name (in A > in B)
   1439    1439 canonicalName;use of name (in A ? in B)
  18571   18291 not in A
    632     244 not in A;use of name (in A ! in B)
    245     194 not in A;use of name (in A < in B)
    208     135 not in A;use of name (in A > in B)
     11       4 not in A;use of name (in A >< in B)
     58      58 not in A;use of name (in A ? in B)
  11906    2074 not in B
    539     162 not in B;use of name (in A ! in B)
    253     215 not in B;use of name (in A < in B)
    242     173 not in B;use of name (in A > in B)
      8       3 not in B;use of name (in A >< in B)
     26      26 not in B;use of name (in A ? in B)
   1617     885 same
   3688    3688 scientificName
     47      47 scientificName;accepted/synonym
    129     129 scientificName;synonym/accepted;use of name (in A ? in B)
    634     634 scientificName;use of name (in A ? in B)
     56      56 synonym/accepted;use of name (in A ? in B)
    732     732 use of name (in A ? in B)
```

(I intend to improve this for better readability; but this is what
it looks like today, 12/31/2021.)

The rows in this table are for categories of records / taxa.  The
first column is the number of taxa (rows, records) in the category.
The second is the number of species [I changed something since this
table was made: it's now the number of _accepted_ species].  The third is a telegraphic
description of the category.  The categories are mutually exclusive.
The description is a set of differences separated by semicolons.

 * `same` means no significant difference between the records was detected.
 * `not in A` and `not in B` mean the taxon occurs
   in only one of the two merge inputs, unmatched to any taxon in the
   other.
 * `canonicalName` and `scientificName` give a column in which the two
   records differ.  `scientificName` implies that the `canonicalName`
   is the same for the A and B records but the `scientificName` differs.
 * `synonym/accepted` means the taxon is given a synonym in the
   'A' input and an accepted name in the 'B' input; `accepted/synonym`
   is the reverse.
 * `use of name` means that the name (from the second input) is used
 * for a different taxon (extension) in A.  `(in A < in B)`
   means the extension for the name in A is inferred to be smaller
   than that for the same name in B, i.e. the application of the name
   has been 'widened'. 


## Reading the output

I use a few conventions for the progress output generated by the
tools, written to stderr:

 - Lines beginning with a run of hyphens `--` are intended to be read by the
   general user, and describe what is going on in terms that are intended to
   be easily understood
 - Lines beginning with asterisk `*` have to do with the detection of bugs or
   peculiar or confusing situations that might demand some attention from a developer
 - Lines beginning with a hash `#` are directed at me, and I don't
   expect anyone else to understand them
 - Other lines come from code that I didn't write, such as `make` or `wc`

