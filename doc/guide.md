# List Tools user guide

**OUT OF DATE**

## Overview

The 'tools' or 'scripts' or 'programs' in this repository manipulate
tabular data in the form of CSV files.  Many of them act as filters
and can be invoked from the Unix shell.  They can be sequenced into
pipelines using '|' if desired.  CSV files here uniformly have a single
header row giving column names.

I have found it convenient to run the tools by using `make` (I use GNU
`make`) applied to a suitable 'makefile'.  This way, intermediate
files can be cached in the local file system, avoiding expensive
regeneration steps when inputs have not changed.

For installation, see [the installation instructions](../INSTALL.md).

## Typical pipeline

A typical processing pipeline to generate an "alignment" style report
would be:

 1. Obtain two DwCA format files.  One way to do this is as follows, but 
    there are others:
     1. Obtain .zip archives, in [DwCA format](file-formats.md), for the two checklists
     1. Find the Taxon file names in the DwCAs using `find_taxa.py` or by 
        examining `meta.xml`
     1. Optional: use `subset.py` to extract the subtree (taxon) of interest
 1. Prepare each checklist as follows so that it is suitable for use with the tools
     1. Normalize for further processing using `clean.py`, saving to a 
        temporary file.
     1. Prepare list of scientific names suitable for use with `gnparse` 
        by applying `extract_names.py` to cleaned checklist.  Pipe to:
     1. Use `gnparse` for parsing, stemming and normalization.  Pipe to:
     1. Fold the `gnparse` output into cleaned checklist with `use_gnparse.py` 
        specifying the temporary file name as a command line argument.
 1. Run `exemplar.py` on the two checklists to obtain exemplars file
 1. Apply [`align.py`](#align) to the checklists and exemplars file to obtain species 
    comparison report (TBD: other kinds of report)

Inputs can be obtained in other formats, such as MDD, NCBI Taxonomy,
or Newick, and converted to DwC.


## The tools

To learn about a given tool, please supplement what follows with
whatever you learn from using the `--help` option, e.g.

    src/clean.py --help

Some tools operate on arbitrary CSV files, while some assume they're
working with Darwin Core files or 'cleaned' Darwin Core files.

### clean

A Darwin Core data source should be run through `src/clean.py` first
for some modest validation and DwC-specific cleaning and normalization.

    src/clean.py --input A.dump/Taxon.tsv >A-clean.csv

`--input` followed by a file name specifies the location of the
uncleaned Darwin Core file.  This is assumed to be in CSV
(comma-separated values) format if the file name ends in `.csv` and
TSV format otherwise.  The output is always CSV.

`--pk columname` specifies the column containing primary keys; it
defaults to `taxonID`.  If there is no such column, one is added, and
if a primary key is missing, a new one is generated by hashing the
contents of the row.

Values in the primary key column that occur there more than once
are detected and flagged.

A 'scientific name' is a 'canonical name' together with an 'authorship'.
Data cleaning is performed for the following columns:
 - If a scientific name (one with
   authorship) is found in the `canonicalName` column, and the
   `scientificName` column is empty, then the canonical name is moved
   to the `scientificName` column.  
 - Similarly, if a canonical name is found in the `scientificName` 
   column, it's moved to the `canonicalName` column.
 - Gven any two of `canonicalName`, `scientificNameAuthorship`, 
   and `scientificName`, the third can be determined.
 - `acceptedNameUsageID` - if it just replicates the `taxonID`, clear it
 - `taxonomicStatus` - report if a record with taxonomic status
   `synonym` does not have an accepted taxon id, or if one with status
   `accepted` does
 - trim leading and trailing space, change double to single space, remove
   trailing period
 - `source` - cleanup specific to EOL DH 0.9 and smasher - remove
   source record ids other than the first
 - `Landmark` - cleanup specific to EOL DH - recode values, change to 
   `landmark_status`

`--managed prefix:column` is for designating use of managed identifier
spaces.  If one column contains, say, NCBI taxids, or GBIF taxon
identifiers, or anything similar, that are stable across versions of
the source checklist (as opposed to being idiosyncratic to one version), then
the is copied to a new `managed_id` column.  For examples, `--managed
gbif:taxonID` means that the taxonID column contains managed GBIF
taxon 'identifiers' and the `managed_id` column will contain 'managed
identifiers'.  E.g. if a row's taxonID contains
`359` then the string `gbif:359` will be placed in the `managed_id`
column.  This will then be used for matching operations (well... not
currently... but it has done so in the past).

### find_taxa

Locates the Darwin Core Taxon file within a .zip file.  Suppose `A.dump/`
is a directory containing the files resulting from unzipping the .zip
file.  Then:

    src/find_taxa.py A.dump

writes the name of the taxon file within the .zip file, e.g. 

    A.dump/Taxon.tsv

This currently operates by examining file names heuristically.
TBD: It really ought to look in the meta.xml file, which provides the
taxon file name explicitly.

It's usually clear by inspecting the file names in A.dump
which file is the taxon file, but
spelling details vary.

The taxon file is suitable as input to `clean.py`:

    src/clean.py `src/find_taxa.py A.dump`


### extract_names

This extracts `scientificName`s in a form suitable for consumption by `gnparser`.
If there is no `scientificName` then the `canonicalName` is extracted.

    src/extract_names.py < A-clean.csv > A-names.txt
    gnparser -s < A-names.txt > A-gnparsed.csv


### use_gnparse

This consumes the output of `gnparser` and combines it with the table
that was the input to `extract_names`, enriching the table with the addition of 
new columns copied from the `gnparser` output.

    src/use_gnparse.py < A-gnparsed.csv > A.csv


### exemplar
<a name="exemplar"></a>

Writes a file giving memberships of exemplars in taxon concepts to
standard output.

 * `--A` filename  - the A checklist.
 * `--B` filename  - the B checklist.

For example,

    src/exemplar.py --A A.csv --B B.csv > AB-exemplars.csv

To do this, noting that each record in each checklist has a taxonomic
name, we posit that each record has a type specimen (the type specimen
for its name).  The type specimen is then an individual that, if found
in taxon concepts in both checklists, can be taken to be an exemplar.

To determine that a type specimen in A is an exemplar we must find a record
in B that has the same type specimen.  That is, the
records must be 'matched'.  (Note that it is records, not their taxon
concepts, that are matched.)  The matcher understands changes in
genus, changes in gender, and other vagaries of the nomenclatural
system leading to name changes, as well as the possibility of collisions (homonyms).
But most of the time name matches are exact.

The meaning of an output row is that the individual (exemplar)
identified by `exemplar id` falls under the taxon concept / record identified
by `taxonID` in the indicated checklist.  (Of course the same exemplar
can also fall under other taxon concepts, in particular taxon concepts
in the other checklist.)

When the exemplars are type specimens, they play a role similar
to that played by protonyms in the Pyle/Remsen formulation.


### align
<a name="align"></a>

    src/align.py --A A.csv --B B.csv --exemplars AB-exemplars.csv

This writes an analysis report to standard output.  Concepts are
imputed for all of the A and B records, and RCC-5 relationships
between those concepts are estimated.

If you're doing regression analysis, think of A as the 'baseline' 
checklist and B as 'proposed successor'.  (This is not the only use case.)

If `--exemplars` is not given, the exemplars are computed just as the
`exemplar.py` command would.

The checklist inputs should be derived through a pipeline that begins with `clean.py`.

When there is a B concept of the same taxonomic
name (allowing for non-semantic changes like genus moves and
spelling corrections) the B concept is called the A concept's "buddy".

The output (to standard output) of `align.py` has these columns (subject to change):
 - `A taxon id` - The taxon id of an A row
 - `A taxon name` - The canonicalName of that A record (for human
   consumption).  A suffixed `*` indicates a synonym.
 - `B taxon id` - The taxon id of a B row
 - `B taxon name` - The canonicalName of that B record (for human
   consumption).  A suffixed `*` indicates a synonym.
 - `A and B` - list of ids of exemplars that occur in both the A
   concept and the name's B concept (the "buddy" concept)
 - `A not B` - list of ids of exemplars that occur in the A
   concept bot not the name's B concept
 - `B not A` - list of ids of exemplars that occur in the 
   concept bot not the name's A concept

Rows are generated for comparison between A species and B species that intersect.

The canonical names in the output are there for human readability.
For a more compact ('normalized') report they might be omitted, and obtained as
needed from the checklists using the taxon id.


### ncbi_to_dwc

Converts an NCBI taxdmp .zip file to DwC.  For example, suppose we fetch
a zip file and unzip it, e.g.:

    wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump_archive/taxdmp_2015-05-01.zip
    unzip -d dump taxdmp_2015-05-01.zip

Then we can convert the dump directory to a single DwC .csv file:

    src/ncbi_to_dwc.py dump >taxdmp_2015-05-01.csv

Columns in the DwC output:
`taxonID`, `NCBI Taxonomy ID`, `parentNameUsageID`, `taxonRank`,
`acceptedNameUsageID`, `scientificName`, `canonicalName`,
`taxonomicStatus`, `nomenclaturalStatus`

Last I checked the dumps from the first of each month are considered
archival.  Others get deleted periodically.

### project

`project` (emphasis on second syllable) drops columns from the input.
With `--keep`, it drops all columns other than those specified:

    src/project.py --keep a,b,c < foo.csv > less-foo.csv

With `--drop`, it drops particular columns, keeping all the rest:

    src/project.py --drop a,b,c < foo.csv > less-foo.csv


### subset

`subset` generates a subset of the rows of a given file, starting from
a specified root.

    src/subset.py --root 40674 < all.csv > some.csv

It assumes the usual Darwin Core hierarchical structure, given by
these columns: 

 - `taxonID` identifies a row (and a taxon, which is probably an extension)
 - `parentNameUsageID` indicates the record for the parent in the hierarchy
 - `acceptedNameUsageID` indicates an accepted name that might replace an
   non-accepted one (although in fact there is more meaning to this attribute).
 - `taxonomicStatus` is `accepted` for an accepted (non-synonym) row, 
   `synonym` for a synonym row

You can specify the root using its `canonicalName` it that's unique:

    src/subset.py --root Mammalia < all.csv > some.csv

### sortcsv

`sortcsv` sorts standard input by the contents of the given `--pk`
(pk = primary key) column, and writes the result to standard output.

    src/sortscv.py --pk taxonID < foo.csv > foo-sorted.csv


### newick

An extremely rudimentary Newick notation parser, intended mainly for
running small tests.

The following accepts a Newick string on the command line and emits a
CSV table with basic Darwin Core columns:

    $ src/newick.py --newick "(a,b)c"
    taxonID,canonicalName,parentNameUsageID,acceptedNameUsageID,taxonomicStatus
    2,a,1,,accepted
    3,b,1,,accepted
    1,c,,,accepted
    $ 

The following reads CSV (with header row) from standard input and
writes a Newick string to standard output:

    $ cat <<EOF | src/newick.py
    taxonID,canonicalName,parentNameUsageID,acceptedNameUsageID,taxonomicStatus
    2,a,1,,accepted
    3,b,1,,accepted
    1,c,,,accepted
    EOF
    (a,b)c
    $ 

Branch length syntax is not handled.  Newick escape sequences are not handled.

This program supports an idiosyncratic syntax for synonyms (useful
since the main use for this feature is testing): an asterisk `*`
suffixed to a name says that the name is to be considered a synonym
(i.e. not accepted).

### eulerx

Work in progress; this hasn't been tested in years.  Produces an
alignment in Euler/X input syntax.

### scatter

Doc FYI