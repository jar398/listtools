# List tools user guide

## Overview

The 'tools' or 'scripts' or 'programs' in this repository manipulate
tabular data in the form of CSV files.  Most of them act as filters
and can be invoked from the Unix shell.  They can be sequenced into
pipelines using '|' if desired.  CSV files uniformly have a single
header row giving column names.

I have found it convenient to run the tools by using `make` applied to
a suitable `makefile'.  This way, intermediate objects can be cached
in the local file system, avoiding expensive regeneration steps when
inputs have not changed.

For a complete example, see [example.md](example.md).

## Installation
<a name="installation" />

The tools require no particular installation step.  They can be
invoked from the shell out of a clone of this `git` repository.

Python 3 is assumed.

Install the python3 `regex` module.

Also required is `gnparser`, which has download instructions
[here](https://github.com/gnames/gnparser#installation).


## The tools

To learn about a given tool, please supplement what follows with
whatever you learn from using the `--help` option, e.g.

    src/clean.py --help

### find_taxa

Locates the Darwin Core taxon file within a .zip file.  Let `A.dump/`
be a directory containing the files resulting from unzipping the .zip
file.  Then:

    src/find_taxa.py A.dump

writes the name of the taxon file within the .zip file, e.g. 

    A.dump/Taxon.tsv

It's usually clear by inspection which file is the taxon file, but
spelling details vary.  This command is intended for use in scripts.

### clean

A Darwin Core data source should be run through `src/clean.py` first
for some modest validation and DwC-specific cleaning.

    src/clean.py --input A.dump/Taxon.tsv >A-clean.csv

`--input` followed by a file name specifies the location of the
uncleaned Darwin Core file.  This is assumed to be in CSV
(comma-separated values) format if the file name ends in `.csv` and
TSV format otherwise.  The output is always CSV.

`--pk columname` specifies the column containing primary keys; it
defaults to `taxonID`.  If there is no such column, one is added, and
if a primary key is missing, a new one is generated by hashing the
contents of the row.

Values in the primary key column that occur there more than once
are detected and flagged.

Data cleaning is performed for the following columns:
 - `canonicalName`, `scientificName` - if a scientific name (one that has an
   authority) is found in the `canonicalName` column, and the
   `scientificName` column is empty, then the scientific name is moved
   to the `scientificName` column.  Similarly, if a non-scientific name
   is found in the `scientificName` column, it's moved to the `canonicalName` column.
 - `acceptedNameUsageID` - if it just replicates the `taxonID`, clear it
 - `taxonomicStatus` - flag if a record with taxonomic status
   `synonym` does not have an accepted taxon id, or if one with status
   `accepted` does
 - `source` - cleanup specific to EOL DH 0.9 and smasher - remove
   source record ids other than the first
 - `Landmark` - recode values, change to `landmark_status` - EOL specific cleanup

`--managed prefix:column` is for designating use of managed identifier
spaces.  If one column contains, say, NCBI taxids, or GBIF taxon
identifiers, or anything similar, that are stable across versions of
the source (as opposed to being idiosyncratic to one version), then
the column should be copied to the `managed_id` column.  This
operation is what this feature is for.  For examples, `--managed
gbif:taxonID` means that the taxonID column contains managed GBIF
taxon 'identifiers' and the `managed_id` column will contain 'managed
identifiers' (an idea I made up).  E.g. if a row's taxonID contains
`359` then the string `gbid:359` will be placed in the `managed_id`
column.  This will then be used for matching operations.


### extract_names

This extracts `scientificName`s in a form suitable for consumption by `gnparser`.
If there is no `scientificName` then the `canonicalName` is extracted.

    src/extract_names < work/A-clean.csv > work/A-names.txt
    gnparser -s < work/A-names.txt > work/A-gnparsed.csv

### use_gnparse

This consumes the output of `gnparser` and combines it with the table
that was just processed, enriching the table with the addition of 
columns from the `gnparser` output.

    src/use_gnparse < work/A-gnparsed.csv > work/A.csv

### exemplar

This is a name matcher.

Given input checklists A and B, finds the best matches between the A
records and the B records.  A set of matched records all have the same type specimen,
and the type specimen of a matched set is called an 'exemplar'.

 * `--A` filename  - the A checklist.
 * `--B` filename  - the B checklist.

Writes exemplar file to standard output.

    src/exemplar --A work/A.csv --B work/B.csv > work/AB-exemplars.csv

There is one output row for each "interesting" A or B checklist row.
 - `checklist`: 0 for the A checklist, 1 for B
 - `taxonID`: the checklist row for a taxon concept
 - `exemplar id`: identifies an exemplar, locally to this analysis (not global).

"Interesting" - TBD: describe what's omitted - many synonyms

The meaning of an output row is that the type specimen of the
indicated taxon concept is the exemplar to be identified by `exemplar
id`.  (The same exemplar can also be the type specimen of other taxon
concepts.)

## plugin

    src/plugin.py --A work/A.csv --B work/B.csv --exemplars work/AB-exemplars.csv

This writes a report to standard output.

If `--exemplars` is not given, the exemplars are computed using `exemplar.py`.

The output (to standard output) has these columns (subject to change):
 - `A taxon id` - the taxon id of an A row
 - `A taxon name` - the canonicalName of that A record (for human consumption)
 - `B species that intersect` - 
   If the A row is for a species, a list of relationships (semicolon 
   separated) for species
   whose taxon concepts are inferred to intersect that A taxon concept.
   Each relationship is given as the RCC-5
   relationship of the A concept to the B concept, along
   with the taxon id and canonical name from the B row.
   A value of '-' means there may be intersecting species but the list was not computed 
   because the A row was not for a species.
 - The relationship to the smallest concept in the B checklist 
   that contains the A row's concept.  The relationship 
   is given as above: RCC-5 relationship, taxon id, canonical name.
   As long as the A
   and B names are accepted, the A concept is either the same (RCC-5 =)
   as the B concept or larger (RCC-5 >) than it.  For synonyms it may
   be hard to tell.

A name written with an asterisk (e.g. `Rana pipiens*`) indicates a synonym.

In case of an ambiguous match, the exemplar set will contain all alternatives.

How to read the report:

If you're mainly concerned with the impact on a data set of advancing
from one version of a checklist to the next (from A to B), then focus
on lines with semicolons, i.e. species intersections with more than
one species.  These rows indicate splits, meaning that data using the
taxon name in A would have to be re-curated to use the correct
intersecting species in B, if one wanted to make the data consistent
with checklist B.

This splits in the report will only be an exhaustive list if enough
synonyms and infraspecific taxa are present in A.  For example, if a
species name is new in B (does not occur in name), then it will look
like a de novo species, rather than a split, if it does not somehow
link back to the split taxon in A via synonym and other records.


[TBD: there should be some indication of name changes; these are hard
to detect on a quick scan otherwise.  Maybe a separate column with this info.]



### ncbi_to_dwc

Converts an NCBI taxdmp .zip file to DwC.  For example, suppose we fetch
a zip file and unzip it, e.g.:

    wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump_archive/taxdmp_2015-05-01.zip
    unzip -d dump taxdmp_2015-05-01.zip

Then we can convert the dump directory to a single DwC .csv file:

    src/ncbi_to_dwc.py dump >taxdmp_2015-05-01.csv

Columns in the DwC output:
`taxonID`, `NCBI Taxonomy ID`, `parentNameUsageID`, `taxonRank`,
`acceptedNameUsageID`, `scientificName`, `canonicalName`,
`taxonomicStatus`, `nomenclaturalStatus`

### project

`project` (emphasis on second syllable) drops columns from the input.
With `--keep`, it drops all columns other than those specified:

    src/project.py --keep a,b,c < foo.csv > less-foo.csv

With `--drop`, it drops particular columns, keeping all the rest:

    src/project.py --drop a,b,c < foo.csv > less-foo.csv


### subset

`subset` generates a subset of the rows of a given file, starting from
a specified root.

    src/subset.py --root 40674 < work/all.csv > work/some.csv

It assumes the usual Darwin Core hierarchical structure, given by
these columns: 

 - `taxonID` identifies a row (and a taxon, which is probably an extension)
 - `parentNameUsageID` indicates the record for the parent in the hierarchy
 - `acceptedNameUsageID` indicates an accepted name that might replace an
   non-accepted one (although in fact there is more meaning to this attribute).
 - `taxonomicStatus` is `accepted` for an accepted (non-synonym) row, 
   `synonym` for a synonym row

You can specify the root using its `canonicalName` it that's unique:

    src/subset.py --root Mammalia < work/all.csv > work/some.csv

### sortcsv

`sortcsv` sorts standard input by the contents of the given `--pk`
(pk = primary key) column, and writes the result to standard output.

    src/sortscv.py --pk taxonID <foo.csv >foo-sorted.csv


### newick

An extremely rudimentary Newick notation parser.  

The following accepts a Newick string on the command line and emits a
CSV table with basic Darwin Core columns:

    $ src/newick.py --newick "(a,b)c"
    taxonID,canonicalName,parentNameUsageID,acceptedNameUsageID,taxonomicStatus
    2,a,1,,accepted
    3,b,1,,accepted
    1,c,,,accepted
    $ 

The following reads CSV (with header row) from standard input and
writes a Newick string to standard output:

    $ cat <<EOF | src/newick.py
    taxonID,canonicalName,parentNameUsageID,acceptedNameUsageID,taxonomicStatus
    2,a,1,,accepted
    3,b,1,,accepted
    1,c,,,accepted
    EOF
    (a,b)c
    $ 

Branch length syntax is not handled.  Newick escape sequences are not handled.

This program supports an idiosyncratic syntax for synonyms (useful
since the main use for this feature is testing): an asterisk `*`
suffixed to a name says that the name is to be considered a synonym
(i.e. not accepted).
